# -*- coding: utf-8 -*-
"""Run.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/yongchao98/NL2TL/blob/main/Run.ipynb
"""

! pip install datasets transformers nltk

from transformers import (AutoModelForSeq2SeqLM,
                          AutoTokenizer,
                          T5Tokenizer)
import torch
import pandas as pd
from datasets import Dataset, DatasetDict, load_dataset, load_from_disk
from tqdm import tqdm

#import subprocess
import sys
import os
import argparse
from IPython.core import error
import random
import numpy as np
import nltk
import json
import csv

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

output_dir = '/content/drive/MyDrive/'
# Link this path in your Google Drive to where your model weights are saved, e.g., checkpoint-62500
# You can download it on the GitHub page

model_checkpoint = "t5-large"
prefix = "Transform the following sentence into Signal Temporal logic: "

max_input_length = 1024
max_target_length = 128
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=max_input_length)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
tl_model = AutoModelForSeq2SeqLM.from_pretrained(
    output_dir + "t5-large-navi-epoch20-trainpoint3"
).to(device)

import time
time_start = time.time()
inputs = [prefix + 'At some point (prop_1), and at some point (prop_2), and always do not (prop_4).']
inputs = tokenizer(
    inputs,
    max_length=max_input_length,
    truncation=True,
    return_tensors="pt"
).to(device)
output = tl_model.generate(**inputs, num_beams=8, do_sample=True, max_length=max_target_length)
decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
print(decoded_output)
time_end = time.time()
print('Translation time: ', time_end - time_start)

# Here are the example test sentences
test_sentence = [
    'Stay at (prop_1) for 5 units in the future and stay at (prop_2) for 5 units in the future, and ensure that never (prop_3).',
    'First (prop_1), and then (prop_2), and ensure that never (prop_3).',
    'Start by (prop_1). Then, (prop_2). Lastly, (prop_3).',
    'Guarantee that you (prop_1) and (prop_2)',  # Input the natural sentence
    '( prop_1 ) and whenever ( prop_2 )',
    'Sooner or later (prop_1)',
    'Repeatedly (prop_1)',
    'At some point, (prop_1).',
    'Do prop_1 but not do prop_2',
    'Do prop_1, do prop_2, do prop_3'
]

for sentence in test_sentence:
    inputs = [prefix + sentence]
    inputs = tokenizer(
        inputs,
        max_length=max_input_length,
        truncation=True,
        return_tensors="pt"
    ).to(device)
    output = tl_model.generate(**inputs, num_beams=8, do_sample=True, max_length=max_target_length)
    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
    print('Input sentence: ', sentence)
    print('Translated STL: ', decoded_output)
    print('\n')

# Here are additional example test sentences
test_sentence = ['At some time travel house or at some time go to apple and pick it']  # Input the natural sentence

for sentence in test_sentence:
    inputs = [prefix + sentence]
    inputs = tokenizer(
        inputs,
        max_length=max_input_length,
        truncation=True,
        return_tensors="pt"
    ).to(device)
    output = tl_model.generate(**inputs, num_beams=8, do_sample=True, max_length=max_target_length)
    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
    print('Input sentence: ', sentence)
    print('Translated STL: ', decoded_output)
    print('\n')

!pip install openai==0.28

import openai
from pyparsing import (
    infixNotation, opAssoc, Keyword, ParserElement, ParseException,
    Regex, Suppress, Optional, Group, Word, nums, OneOrMore, Combine
)

OPENAI_API_KEY = "Your-API-KEY"
openai.api_key = OPENAI_API_KEY
# Enable packrat acceleration for parsing
ParserElement.enablePackrat()

# Define interval expressions, e.g., [0,5]
interval = Group(
    Suppress("[") + Word(nums)("start") + Suppress(",") + Word(nums)("end") + Suppress("]")
)

# Define keywords (operators)
imply_kw    = Keyword("imply")
equal_kw    = Keyword("equal")
and_kw      = Keyword("and")
or_kw       = Keyword("or")
finally_kw  = Keyword("finally")
globally_kw = Keyword("globally")
until_kw    = Keyword("until")
negation_kw = Keyword("negation")

# Helper function: flatten operator with optional interval
def flatten_operator(tokens):
    op = tokens[0]
    if len(tokens) > 1 and tokens[1]:
        intv = tokens[1]
        return f"{op}[{intv['start']},{intv['end']}]"
    else:
        return op

# Define temporal operators with optional intervals
globally_temporal = (globally_kw + Optional(interval)).setParseAction(flatten_operator)
finally_temporal  = (finally_kw + Optional(interval)).setParseAction(flatten_operator)
until_temporal    = (until_kw + Optional(interval)).setParseAction(flatten_operator)

# Define other operators directly
imply_op     = imply_kw
equal_op     = equal_kw
and_op       = and_kw
or_op        = or_kw
negation_op  = negation_kw

# Define AP (atomic propositions)
AP = Regex(
    r"(?!^(?:imply|equal|and|or|finally|globally|until|negation)$)[A-Za-z0-9_ ]+"
).setName("AP").addParseAction(lambda t: t[0].strip())

# Construct expression: use infixNotation to define unary and binary operators
expr = infixNotation(
    AP,
    [
        (negation_op, 1, opAssoc.RIGHT),
        ((globally_temporal | finally_temporal), 1, opAssoc.RIGHT),
        (until_temporal, 2, opAssoc.RIGHT),
        ((and_op | or_op | imply_op | equal_op), 2, opAssoc.RIGHT),
    ]
)

def is_unary_op(token):
    for op in ["negation", "globally", "finally"]:
        if token.startswith(op):
            return True
    return False

def is_binary_op(token):
    for op in ["imply", "equal", "and", "or", "until"]:
        if token.startswith(op):
            return True
    return False

def check_tree(tree):
    if isinstance(tree, str):
        return bool(tree.strip())
    if isinstance(tree, list):
        if len(tree) == 1:
            return check_tree(tree[0])
        if len(tree) == 2:
            op, operand = tree
            if isinstance(op, str) and is_unary_op(op):
                return check_tree(operand)
            return False
        if len(tree) == 3:
            left, op, right = tree
            if isinstance(op, str) and is_binary_op(op):
                return check_tree(left) and check_tree(right)
            return False
        return False
    return False

# Validate STL expression function: returns (True, "") or (False, error_message)
def validate_stl(stl_expression):
    try:
        result = expr.parseString(stl_expression, parseAll=True)
        tree = result.asList()
        if check_tree(tree):
            return (True, "")
        else:
            return (False, "Structure check failed")
    except ParseException as pe:
        return (False, str(pe))

# Auto-correct STL function, includes formal rule descriptions (passed in the prompt)
def auto_correct_stl(nl, stl, error_message):
    # Here you can include detailed formal rule descriptions in the prompt
    formal_rules = (
        "正式规则：\n"
        "1. 每个二元操作符必须有两个操作数，并且左右操作数必须用括号包围。\n"
        "2. 时间区间必须紧跟在 temporal 操作符后面，格式为 [start,end]，后面必须跟括号包围的子公式。\n"
        "3. 括号必须完全匹配。\n"
        "请将错误的 STL 修正为符合上述规则的正确 STL 表达式，且保持所有操作符原样。"
    )
    prompt = (
        f"原自然语言描述: {nl}\n"
        f"错误的 STL: {stl}\n"
        f"解析错误信息: {error_message}\n"
        f"{formal_rules}\n"
        "请仅输出修正后的 STL 表达式，不附带其他解释。"
    )
    try:
        response = openai.ChatCompletion.create(
            model='o3-mini',
            messages=[
                {"role": "system",
                 "content": "你是一个时序逻辑专家，擅长根据正式规则修正 STL 表达式。请将错误 STL 修正为既符合语法又符合语义的正确 STL 表达式，且保持所有操作符的原始英文单词。"},
                {"role": "user", "content": prompt}
            ],
            reasoning_effort='high'
        )
        corrected = response.choices[0].message['content'].strip()
        return corrected
    except Exception as e:
        print("OpenAI API error:", e)
        return stl

# API_MODE controls whether to always call the auto-correction module ("always" or "only_if_error")
API_MODE = "only_if_error"

# Assume the following variables are defined: prefix, tokenizer, tl_model, max_input_length, max_target_length, device
# Below is an example of the integrated invocation:
test_sentence = ['At some time travel house or at some time go to apple and pick it']
for sentence in test_sentence:
    # Coreference detection and resolution
    if contains_pronoun(sentence):
        resolved_sentence = resolve_coreference(sentence)
        sentence = resolved_sentence
        print("Coreference resolved sentence:", sentence)

    # Translation step (ensure prefix, tokenizer, tl_model, device are defined)
    inputs = [prefix + sentence]
    inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors="pt").to(device)
    output = tl_model.generate(**inputs, num_beams=8, do_sample=True, max_length=max_target_length)
    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]

    # STL validation
    valid, error_message = validate_stl(decoded_output)
    if API_MODE == "always" or (API_MODE == "only_if_error" and not valid):
        final_stl = auto_correct_stl(sentence, decoded_output, error_message)
    else:
        final_stl = decoded_output

    # Concise output
    print("Input sentence: ", sentence)
    print("Translated STL: ", decoded_output)
    print("Final STL: ", final_stl)
    print('\n')
